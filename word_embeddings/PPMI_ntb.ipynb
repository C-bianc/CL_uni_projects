{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8269fcc8-7242-4357-ab35-79e2344197ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "#===============================================================================\n",
    "#\n",
    "#           FILE: PPMI_ntb.py \n",
    "#         AUTHOR: Bianca Ciobanica\n",
    "#\t       EMAIL: bianca.ciobanica@student.uclouvain.be\n",
    "#\n",
    "#           BUGS: \n",
    "#        VERSION: 3.11.4\n",
    "#        CREATED: 13-11-2023 \n",
    "#\n",
    "#===============================================================================\n",
    "#    DESCRIPTION:  sources used : \n",
    "#                  https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html\n",
    "#                  https://jofrhwld.github.io/teaching/courses/2022_lin517/lectures/word_vectors/02_vectors_examples.html\n",
    "#                  https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.dok_matrix.html\n",
    "#                  https://numpy.org/doc/stable/reference/generated/numpy.clip.html\n",
    "#                  https://stackoverflow.com/questions/3391843/how-to-transform-negative-elements-to-zero-without-a-loop\n",
    "#    \n",
    "#          USAGE: \n",
    "#==============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64d7a375-822c-4919-9555-640aca856f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "from itertools import chain\n",
    "import math\n",
    "from nltk.corpus.reader import PlaintextCorpusReader\n",
    "from nltk.lm import Vocabulary\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cad9a0f1-233d-44c7-bd21-869151237115",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time_program = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cccb3e03-1c60-4588-bd87-c3ad7be08117",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = PlaintextCorpusReader(root=\".\", \n",
    "                               fileids=[\"corpus.txt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fe766a8-bbfc-47ed-acd2-d3c0accbfff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(corpus.raw()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "905a53cc-f5d5-4819-87b7-8ae3c1580d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_steps(corpus):\n",
    "    # get words\n",
    "    text = corpus.sents()\n",
    "    \n",
    "    processed_text = [\n",
    "        [token.lower() for token in re.sub(r\"[.,:;!?\\-\\'\\\"\\(\\)\\[\\]]+\", ' ', \" \".join(sentence)).split() if token != \"\"]\n",
    "        for sentence in text\n",
    "    ]\n",
    "\n",
    "    words = list(chain.from_iterable(processed_text))\n",
    "    return words\n",
    "    \n",
    "preprocessed_words = preprocess_steps(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "738721f1-c553-4291-8366-76e6a8dc1f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~~~~~ Create restricted voc ~~~~~\n",
    "unk_cutoff = 10\n",
    "#freqDist = Counter(sorted(words)) # sort alphabetically beforehand\n",
    "\n",
    "#top_5_words = [item[0] for item in freqDist.most_common()[:5]] # get top 5\n",
    "#print(\"\\n\".join(top_5_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f90cf1ce-ae14-4808-af06-8e8ba7eb76b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_restricted_voc(words, threshold=None):\n",
    "    word_counts = Counter(words)\n",
    "    \n",
    "    restricted_voc = ['<UNK>' if word_counts[word] < threshold else word for word in words]\n",
    "    \n",
    "    unique_tokens = set(restricted_voc)\n",
    "    \n",
    "    return restricted_voc, unique_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71aff007-f8d3-4f6e-9451-097fba150509",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index(corpus, unique_tokens):\n",
    "    words_index = {word: [] for word in unique_tokens}\n",
    "\n",
    "    for idx, word in enumerate(corpus):\n",
    "        words_index[word].append(idx)\n",
    "    \n",
    "    return words_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b86633fc-e8b7-4959-a14b-8ce633f542e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "words, unique_tokens = create_restricted_voc(preprocessed_words, threshold=unk_cutoff) # oov words replaced with unk in words array\n",
    "voc_index = {word: idx for idx,word in enumerate (unique_tokens)}\n",
    "words_index = create_index(words, unique_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea9014d6-f88f-4159-9d0f-d179f933f2ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique tokens : 15200\n"
     ]
    }
   ],
   "source": [
    "voc_size = len(unique_tokens)\n",
    "print(\"unique tokens :\",voc_size)\n",
    "\n",
    "alpha = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6858f5f-2850-46d8-b423-4f6de7232b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~~~~~ Contextual windows  ~~~~~\n",
    "\n",
    "def window(i):\n",
    "    if i == 0: # first word\n",
    "        return [words[i+1], words[i+2]] \n",
    "    if len(words) - i == 2: # penultimate word\n",
    "        return [words[i-2], words[i-1], words[i+1]]\n",
    "    if len(words) - i == 1: # last word\n",
    "        return [words[i-2], words[i-1]]\n",
    "                \n",
    "    return [words[i-2], words[i-1], words[i+1], words[i+2]]\n",
    "    \n",
    "#print(\",\".join(window(0)))\n",
    "#print(\",\".join(window(words.index(\"person\"))))\n",
    "#print(\",\".join(window(len(words) - 2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9fb89cf7-4749-426b-a041-35625b0ac2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def co_occurrence_matrix_sparse(target_words):\n",
    "    start_time = time.time()\n",
    "    matrix = {target_word: Counter() for target_word in target_words} \n",
    "    \n",
    "    for word in target_words:\n",
    "        # get co-occurence vector for target word\n",
    "        for idx in target_words[word]:\n",
    "            target_context_words = window(idx)\n",
    "            \n",
    "            # update counts\n",
    "            matrix[word].update(target_context_words) # we feed the context words to the counter object\n",
    "                \n",
    "    matrix_data_dict = list(matrix.values()) # is a list of counters, so word => vector \n",
    "\n",
    "    # create a df from iterable dict\n",
    "    occurences_matrix = pd.DataFrame(matrix_data_dict, index=list(target_words), columns=list(target_words))\n",
    "    occurences_matrix = occurences_matrix.fillna(0).astype(int) # fill 0 for words not occuring with others\n",
    "\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    \n",
    "    print(f\"Execution time: {execution_time} seconds\")\n",
    "\n",
    "    return occurences_matrix\n",
    "    \n",
    "#term_context_matrix_sparse = co_occurrence_matrix_sparse(words_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b2ff1bb-08aa-45c4-ae63-1b45f639e661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# P(w_i, c_j) = C(w_i, c_j) + alpha / sum( C(w_i,c_j) + alpha )\n",
    "# P(w_i) = sum_c_j ( C(w_i, c_j )\n",
    "# P(c_j) = sum_w_i ( C(w_i, c_j )\n",
    "def create_ppmi_sparse(matrix, alpha):\n",
    "    total_counts = sum((matrix + alpha).sum())\n",
    "\n",
    "    mle_matrix =  (matrix + alpha) / total_counts\n",
    "    p_wi = np.sum(matrix, axis=1)/ total_counts\n",
    "    p_cj = np.sum(matrix, axis=0) / total_counts\n",
    "    ppmi_sparse = np.log2(mle_matrix / ( p_wi  *  p_cj ))\n",
    "    ppmi_sparse = ppmi_sparse.clip(0)\n",
    "    \n",
    "    return ppmi_sparse\n",
    "    \n",
    "#ppmi_sparse = create_ppmi_sparse(term_context_matrix_sparse, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d65507f-8afc-4290-8c76-0c775e0a15f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 2.4832537174224854 seconds\n"
     ]
    }
   ],
   "source": [
    "def co_occurrence_matrix(target_words):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    matrix = {target_word: Counter() for target_word in target_words} \n",
    "    \n",
    "    for word in target_words:\n",
    "        # get co-occurence vector for target word\n",
    "        for idx in target_words[word]: # position in original corpus\n",
    "            target_context_words = window(idx)\n",
    "            \n",
    "            # update counts\n",
    "            matrix[word].update(target_context_words) # we feed the context words to the counter object\n",
    "\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    \n",
    "    print(f\"Execution time: {execution_time} seconds\")\n",
    "    return matrix\n",
    "    \n",
    "term_context_matrix = co_occurrence_matrix(words_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c3bebbc-2699-4bc0-93ce-4364a17466f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_total_MLE(matrix):\n",
    "    mle = 0\n",
    "    for counter in matrix.values():\n",
    "        mle += sum(count for count in counter.values())\n",
    "            \n",
    "    return mle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc33e4e1-4c97-4e79-92f5-6d8c0faec150",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_p_cj(matrix):\n",
    "    p_cj = 0\n",
    "    context_counts = Counter()\n",
    "\n",
    "    for word, context_counter in matrix.items():\n",
    "        # add each key which is the context word and its count to the counter\n",
    "        context_counts += context_counter \n",
    "    return context_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96602892-70ec-4742-90ce-c51876f8295c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9816888.52\n",
      "Execution time: 10.765803813934326 seconds\n"
     ]
    }
   ],
   "source": [
    "# P(w_i, c_j) = C(w_i, c_j) + alpha / sum( C(w_i,c_j) + alpha )\n",
    "# P(w_i) = sum_c_j ( C(w_i, c_j )\n",
    "# P(c_j) = sum_w_i ( C(w_i, c_j )\n",
    "\n",
    "def create_ppmi(matrix, alpha):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    total_context_counts = calculate_total_MLE(matrix) + (alpha * len(matrix))\n",
    "    print(total_context_counts)\n",
    "    \n",
    "    context_counts = calculate_p_cj(matrix)\n",
    "    # or mle = sum(count for counter in term_context_matrix.values() for count in counter.values()) + ( alpha * (voc_size **2))\n",
    "\n",
    "    for word in matrix:\n",
    "        p_wi = sum(matrix[word].values()) / total_context_counts # w_i frequency\n",
    "        \n",
    "        for context_w, count in matrix[word].items():\n",
    "            p_wi_cj = (count + alpha) / total_context_counts # p(context_j co-occuring with target w_i)\n",
    "            p_cj = context_counts[context_w] / total_context_counts\n",
    "            ppmi  = math.log2(p_wi_cj / (p_wi * p_cj))\n",
    "            matrix[word][context_w] = max(ppmi, 0)\n",
    "\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    \n",
    "    print(f\"Execution time: {execution_time} seconds\")\n",
    "    return matrix\n",
    "\n",
    "ppmi_matrix = create_ppmi(term_context_matrix, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "444eef08-2501-435e-be9f-181f9b4141a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cosine_sim(word1, word2):\n",
    "    \"\"\" input : two vectors of two words\n",
    "        return : cosine similarity value\n",
    "    \"\"\"\n",
    "    vector1 = ppmi_matrix[word1]\n",
    "    vector2 = ppmi_matrix[word2]\n",
    "\n",
    "    dot_product = 0\n",
    "\n",
    "    common_words = set(vector1.keys()) & set(vector2.keys())\n",
    "    for word in common_words:\n",
    "        dot_product += vector1[word]  * vector2[word]\n",
    "\n",
    "    v1_norm = np.linalg.norm(list(c for c in vector1.values()))\n",
    "    v2_norm = np.linalg.norm(list(c for c in vector2.values()))\n",
    "    # or without numpy\n",
    "    #v1_norm = math.sqrt(sum(map(lambda x: x*x, vector1.values())))\n",
    "    #v2_norm = math.sqrt(sum(map(lambda x: x*x, vector2.values())))\n",
    "    total_counts = v1_norm * v2_norm\n",
    "\n",
    "    normalized_dotprod = dot_product / total_counts\n",
    "    \n",
    "    return normalized_dotprod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "af882e71-c1f3-4073-b96d-dbbbefac07d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cosine_sparse(word1,word2):\n",
    "\n",
    "    vector1 = ppmi_sparse.loc[word1]\n",
    "    vector2 = ppmi_sparse.loc[word2]\n",
    "    \n",
    "    \n",
    "    dot_product = np.inner(vector1, vector2)\n",
    "\n",
    "    v1_norm = np.linalg.norm(vector1)\n",
    "    v2_norm = np.linalg.norm(vector2)\n",
    "\n",
    "    \n",
    "    # or without numpy\n",
    "    #v1_norm = math.sqrt(sum(map(lambda x: x*x, vector1)))\n",
    "    #v2_norm = math.sqrt(sum(map(lambda x: x*x, vector2)))\n",
    "    total_counts = v1_norm * v2_norm\n",
    "\n",
    "    normalized_dotprod = dot_product / total_counts\n",
    "\n",
    "    return normalized_dotprod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c630e2ac-378a-4a8d-a2e7-494c61b2d5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_5_closest_words(target):\n",
    "    similarities = {}\n",
    "    #similarities_sparse = {}\n",
    "    \n",
    "    for w in unique_tokens:\n",
    "        if w == target:\n",
    "            continue\n",
    "        sim = get_cosine_sim(target,w)\n",
    "        #sim_sparse = get_cosine_sparse(target,w)\n",
    "\n",
    "        similarities[w] = sim\n",
    "       # similarities_sparse[w] = sim_sparse\n",
    "                                         \n",
    "    top_5_words = sorted(similarities, key=similarities.get, reverse=True)[:5]\n",
    "    #top_5_words_sparse = sorted(similarities, key=similarities.get, reverse=True)[:5]\n",
    "    \n",
    "    print(\"Top 5 words similar to \" + target)\n",
    "    for w in top_5_words:\n",
    "        print(f\"{w}: {similarities[w]}\")\n",
    "    #print(\"From scratch and no sparse\".ljust(50), \"Sparse matrix\\n\")\n",
    "    #for w, w_sparse in zip(top_5_words, top_5_words_sparse):\n",
    "     #   print(f\"{w}: {similarities[w]}\".ljust(50), \n",
    "    #        f\"{w_sparse}: {similarities_sparse[w_sparse]}\")\n",
    "    print()\n",
    "    \n",
    "    return set(top_5_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "45791eb4-bcda-44cd-b9c8-62987e3d05e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 words similar to car\n",
      "cars: 0.17545540185356884\n",
      "vehicles: 0.13381011499813825\n",
      "electric: 0.13203808511780504\n",
      "fuel: 0.1146915686314566\n",
      "a: 0.11134307878896335\n",
      "\n",
      "Top 5 words similar to feature\n",
      "features: 0.12279135136721567\n",
      "film: 0.10958183870864024\n",
      "most: 0.10227191575051996\n",
      "roles: 0.10030301352910866\n",
      "films: 0.09936376474575513\n",
      "\n",
      "Top 5 words similar to computer\n",
      "graphics: 0.14414894689831118\n",
      "software: 0.1384913207106095\n",
      "technology: 0.13741492287690896\n",
      "system: 0.13550690971258733\n",
      "programming: 0.13514113451579493\n",
      "\n"
     ]
    }
   ],
   "source": [
    "car_closest = get_5_closest_words('car')\n",
    "feature_closest = get_5_closest_words('feature')\n",
    "computer_closest = get_5_closest_words('computer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a89fd52a-c3c1-4e4f-a6fc-3cec0131e01a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 closest words to car: \n",
      " {'electric', 'vehicles', 'fuel', 'a', 'cars'}\n",
      "5 closest words to feature: \n",
      " {'most', 'features', 'film', 'films', 'roles'}\n",
      "5 closest words to computer: \n",
      " {'technology', 'programming', 'graphics', 'software', 'system'}\n"
     ]
    }
   ],
   "source": [
    "print(\"5 closest words to car: \\n\", car_closest)\n",
    "print(\"5 closest words to feature: \\n\",feature_closest)\n",
    "print(\"5 closest words to computer: \\n\",computer_closest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "219129a9-d350-445d-8eaa-771e299f05fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total execution time: 20.51163363456726 seconds\n"
     ]
    }
   ],
   "source": [
    "end_time_program = time.time()\n",
    "total_execution_time = end_time_program - start_time_program\n",
    "    \n",
    "print(f\"Total execution time: {total_execution_time} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
