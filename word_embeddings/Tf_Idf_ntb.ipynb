{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8269fcc8-7242-4357-ab35-79e2344197ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "#===============================================================================\n",
    "#\n",
    "#           FILE: Tf_Idf_ntb.py \n",
    "#         AUTHOR: Bianca Ciobanica\n",
    "#\t       EMAIL: bianca.ciobanica@student.uclouvain.be\n",
    "#\n",
    "#           BUGS: \n",
    "#        VERSION: 3.11.4\n",
    "#        CREATED: 13-11-2023 \n",
    "#\n",
    "#===============================================================================\n",
    "#    DESCRIPTION:  sources used : \n",
    "#                  https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html\n",
    "#                  https://jofrhwld.github.io/teaching/courses/2022_lin517/lectures/word_vectors/02_vectors_examples.html\n",
    "#                  https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.dok_matrix.html\n",
    "#                  https://docs.python.org/3/library/stdtypes.html#frozenset.union\n",
    "#    \n",
    "#          USAGE: \n",
    "#==============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64d7a375-822c-4919-9555-640aca856f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "from itertools import chain\n",
    "import math\n",
    "from nltk.corpus.reader import PlaintextCorpusReader\n",
    "from nltk.lm import Vocabulary\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c066b204-04c2-4338-adce-3a68f952f61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time_program = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cccb3e03-1c60-4588-bd87-c3ad7be08117",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = PlaintextCorpusReader(root=\".\", \n",
    "                               fileids=[\"corpus.txt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fe766a8-bbfc-47ed-acd2-d3c0accbfff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(corpus.raw()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "905a53cc-f5d5-4819-87b7-8ae3c1580d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_steps(corpus):\n",
    "    # get words\n",
    "    text = corpus.sents()\n",
    "    \n",
    "    processed_text = [\n",
    "        [token.lower() for token in re.sub(r\"[.,:;!?\\-\\'\\\"\\(\\)\\[\\]]+\", ' ', \" \".join(sentence)).split() if token != \"\"]\n",
    "        for sentence in text\n",
    "    ]\n",
    "\n",
    "    words = list(chain.from_iterable(processed_text))\n",
    "    \n",
    "    return words\n",
    "    \n",
    "words = preprocess_steps(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "738721f1-c553-4291-8366-76e6a8dc1f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~~~~~ Create restricted voc ~~~~~\n",
    "unk_cutoff = 10\n",
    "#freqDist = Counter(sorted(words)) # sort alphabetically beforehand\n",
    "\n",
    "#top_5_words = [item[0] for item in freqDist.most_common()[:5]] # get top 5\n",
    "#print(\"\\n\".join(top_5_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f90cf1ce-ae14-4808-af06-8e8ba7eb76b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_restricted_voc(threshold=None):\n",
    "    word_counts = Counter(words)\n",
    "    \n",
    "    restricted_voc = ['<UNK>' if word_counts[word] < threshold else word for word in words]\n",
    "    \n",
    "    unique_tokens = list(set(restricted_voc))\n",
    "    \n",
    "    return restricted_voc, unique_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71aff007-f8d3-4f6e-9451-097fba150509",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index(corpus, unique_tokens):\n",
    "    words_index = {word: [] for word in unique_tokens}\n",
    "    \n",
    "    for idx, word in enumerate(corpus):\n",
    "        words_index[word].append(idx)\n",
    "    \n",
    "    return words_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b86633fc-e8b7-4959-a14b-8ce633f542e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "words, unique_tokens = create_restricted_voc(threshold=unk_cutoff) # oov words replaced with unk in words array\n",
    "words_index = create_index(words, unique_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea9014d6-f88f-4159-9d0f-d179f933f2ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique tokens : 15200\n"
     ]
    }
   ],
   "source": [
    "voc_size = len(unique_tokens)\n",
    "print(\"unique tokens :\",voc_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6858f5f-2850-46d8-b423-4f6de7232b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~~~~~ Contextual windows  ~~~~~\n",
    "# w1 = words[0]\n",
    "# w2 = computer\n",
    "# w3 = words[-2]\n",
    "def window(i):\n",
    "    if i == 0: # first word\n",
    "        return [words[i+1], words[i+2]] \n",
    "    if len(words) - i == 2: # penultimate word\n",
    "        return [words[i-2], words[i-1], words[i+1]]\n",
    "    if len(words) - i == 1: # last word\n",
    "        return [words[i-2], words[i-1]]\n",
    "                \n",
    "    return [words[i-2], words[i-1], words[i+1], words[i+2]]\n",
    "    \n",
    "#print(\",\".join(window(0)))\n",
    "#print(\",\".join(window(words.index(\"person\"))))\n",
    "#print(\",\".join(window(len(words) - 2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "169e346a-86e4-4b6a-a98a-b590904095f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 1.8197307586669922 seconds\n"
     ]
    }
   ],
   "source": [
    "def co_occurrence_matrix(target_words):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    matrix = {target_word: Counter() for target_word in target_words} \n",
    "    \n",
    "    for word in target_words:\n",
    "        # get co-occurence vector for target word\n",
    "        for idx in target_words[word]: # position in original corpus\n",
    "            target_context_words = window(idx)\n",
    "            \n",
    "            # update counts\n",
    "            matrix[word].update(target_context_words) # we feed the context words to the counter object\n",
    "\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    \n",
    "    print(f\"Execution time: {execution_time} seconds\")\n",
    "    return matrix\n",
    "    \n",
    "term_context_matrix = co_occurrence_matrix(words_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b0d9b4a-1d38-4d07-813c-f81558336434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 0.4287431240081787 seconds\n"
     ]
    }
   ],
   "source": [
    "def create_tfidf_matrix(matrix):\n",
    "    start_time = time.time()\n",
    "    \"\"\" \n",
    "        input : non sparse matrix where matrix is a dict of dicts\n",
    "                each word has a counter (its vector)\n",
    "        returns : tfidf matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    # tf_d = log10(count(t, d) + 1)\n",
    "    # idf_d =  log10 (n = V size / df)\n",
    "    for word in matrix:\n",
    "        for cword, count in matrix[word].items():\n",
    "            tf = math.log10(count + 1)\n",
    "            df = len( matrix[word])\n",
    "            idf = math.log10(voc_size / df)\n",
    "            matrix[word][cword] = tf*idf\n",
    "\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    \n",
    "    print(f\"Execution time: {execution_time} seconds\")\n",
    "    \n",
    "    return matrix\n",
    "\n",
    "tfidf_matrix = create_tfidf_matrix(term_context_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c95c12d-36cd-49c0-8244-9a05a9d7c8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def co_occurrence_matrix_sparse(target_words):\n",
    "    start_time = time.time()\n",
    "    matrix = {target_word: Counter() for target_word in target_words} \n",
    "    \n",
    "    for word in target_words:\n",
    "        # get co-occurence vector for target word\n",
    "        for idx in target_words[word]: # position in original corpus\n",
    "            target_context_words = window(idx)\n",
    "            \n",
    "            # update counts\n",
    "            matrix[word].update(target_context_words) # we feed the context words to the counter object\n",
    "\n",
    "    matrix_data_dict = list(matrix.values()) # is a list of counters, so word => vector \n",
    "\n",
    "    # create a df from iterable dict\n",
    "    occurences_matrix = pd.DataFrame(matrix_data_dict, index=list(target_words), columns=list(target_words))\n",
    "    occurences_matrix = occurences_matrix.fillna(0).astype(int) # fill 0 for words not occuring with others (no!!!)\n",
    "\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    \n",
    "    print(f\"Execution time: {execution_time} seconds\")\n",
    "    return occurences_matrix\n",
    "    \n",
    "#term_context_matrix_sparse = co_occurrence_matrix_sparse(words_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47866595-6399-497a-a437-f313ef2fe19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tfidf_sparse(matrix):\n",
    "    # tf_d = log10(count(t, d) + 1)\n",
    "    # idf_d =  log10 (n = V size / df)\n",
    "    tf_matrix_sparse = np.log10(matrix + 1)\n",
    "\n",
    "    #calculate df, we multiply each frequency by the vector df_i\n",
    "    df_vector_sparse = np.asarray((tf_matrix_sparse > 0).sum(axis=1))\n",
    "    idf_vector_sparse = np.log10(voc_size / (df_vector_sparse ))\n",
    "    \n",
    "    tfidf_matrix_sparse = tf_matrix_sparse.mul(idf_vector_sparse, axis=0)\n",
    "\n",
    "    return tfidf_matrix_sparse\n",
    "#tfidf_matrix_sparse = create_tfidf_sparse(term_context_matrix_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "444eef08-2501-435e-be9f-181f9b4141a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cosine_sim(word1, word2):\n",
    "    \"\"\" input : two vectors of two words\n",
    "        return : cosine similarity value\n",
    "    \"\"\"\n",
    "    vector1 = tfidf_matrix[word1]\n",
    "    vector2 = tfidf_matrix[word2]\n",
    "    \n",
    "    dot_product = 0\n",
    "\n",
    "    common_words = set(vector1.keys()) & set(vector2.keys())\n",
    "    for word in common_words:\n",
    "        dot_product += vector1[word]  * vector2[word]\n",
    "\n",
    "    v1_norm = np.linalg.norm(list(c for c in vector1.values()))\n",
    "    v2_norm = np.linalg.norm(list(c for c in vector2.values()))\n",
    "    # or without numpy\n",
    "    #v1_norm = math.sqrt(sum(map(lambda x: x*x, vector1.values())))\n",
    "    #v2_norm = math.sqrt(sum(map(lambda x: x*x, vector2.values())))\n",
    "    total_counts = v1_norm * v2_norm\n",
    "\n",
    "    normalized_dotprod = dot_product / total_counts\n",
    "    \n",
    "    return normalized_dotprod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cff4a5a0-643c-4c91-b4b3-ac375fbd9325",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cosine_sparse(word1,word2):\n",
    "\n",
    "    vector1 = tfidf_matrix_sparse.loc[word1]\n",
    "    vector2 = tfidf_matrix_sparse.loc[word2]\n",
    "\n",
    "    dot_product = np.inner(vector1, vector2)\n",
    "\n",
    "    v1_norm = np.linalg.norm(vector1)\n",
    "    v2_norm = np.linalg.norm(vector2)\n",
    "\n",
    "    # or without numpy\n",
    "    #v1_norm = math.sqrt(sum(map(lambda x: x*x, vector1)))\n",
    "    #v2_norm = math.sqrt(sum(map(lambda x: x*x, vector2)))\n",
    "    total_counts = v1_norm * v2_norm\n",
    "\n",
    "    normalized_dotprod = dot_product / total_counts\n",
    "    \n",
    "    return normalized_dotprod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c630e2ac-378a-4a8d-a2e7-494c61b2d5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_5_closest_words(target):\n",
    "    similarities = {}\n",
    "   # similarities_sparse = {}\n",
    "    \n",
    "    for w in unique_tokens:\n",
    "        if w == target:\n",
    "            continue\n",
    "        sim = get_cosine_sim(target,w)\n",
    "       # sim_sparse = get_cosine_sparse(target,w)\n",
    "\n",
    "        similarities[w] = sim\n",
    "      #  similarities_sparse[w] = sim_sparse\n",
    "                                         \n",
    "    top_5_words = sorted(similarities, key=similarities.get, reverse=True)[:5]\n",
    "    #top_5_words_sparse = sorted(similarities, key=similarities.get, reverse=True)[:5]\n",
    "    \n",
    "    print(\"Top 5 words similar to \" + target)\n",
    "    print(\"From scratch and no sparse\")\n",
    "   # print(\"From scratch and no sparse\".ljust(50), \"Sparse matrix\\n\")\n",
    "    for w in top_5_words:\n",
    "        print(f\"{w}: {similarities[w]}\")\n",
    "   # for w, w_sparse in zip(top_5_words, top_5_words_sparse):\n",
    "   #     print(f\"{w}: {similarities[w]}\".ljust(50), \n",
    "    #          f\"{w_sparse}: {similarities_sparse[w_sparse]}\")\n",
    "    print()\n",
    "    \n",
    "    return set(top_5_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "45791eb4-bcda-44cd-b9c8-62987e3d05e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 words similar to car\n",
      "From scratch and no sparse\n",
      "cars: 0.5459432013009844\n",
      "issue: 0.5350757773255332\n",
      "built: 0.5338796657113848\n",
      "position: 0.5327571649308895\n",
      "meeting: 0.5292846421623635\n",
      "\n",
      "Top 5 words similar to feature\n",
      "From scratch and no sparse\n",
      "issue: 0.5509324369691192\n",
      "event: 0.5497552268988785\n",
      "appearance: 0.548460352322813\n",
      "featured: 0.5469790860048481\n",
      "features: 0.5417038249595428\n",
      "\n",
      "Top 5 words similar to computer\n",
      "From scratch and no sparse\n",
      "technology: 0.5178624163693614\n",
      "software: 0.5084677660785514\n",
      "system: 0.4878365169166404\n",
      "programming: 0.4847636197099709\n",
      "game: 0.4830950880372917\n",
      "\n"
     ]
    }
   ],
   "source": [
    "car_closest = get_5_closest_words('car')\n",
    "feature_closest = get_5_closest_words('feature')\n",
    "computer_closest = get_5_closest_words('computer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a89fd52a-c3c1-4e4f-a6fc-3cec0131e01a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 closest words to car: \n",
      " {'issue', 'position', 'meeting', 'built', 'cars'}\n",
      "5 closest words to feature: \n",
      " {'event', 'featured', 'issue', 'features', 'appearance'}\n",
      "5 closest words to computer: \n",
      " {'technology', 'programming', 'system', 'software', 'game'}\n"
     ]
    }
   ],
   "source": [
    "print(\"5 closest words to car: \\n\", car_closest)\n",
    "print(\"5 closest words to feature: \\n\",feature_closest)\n",
    "print(\"5 closest words to computer: \\n\",computer_closest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "15933759-2dc6-4b16-873d-a7e7a5813f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total execution time: 8.554362058639526 seconds\n"
     ]
    }
   ],
   "source": [
    "end_time_program = time.time()\n",
    "total_execution_time = end_time_program - start_time_program\n",
    "    \n",
    "print(f\"Total execution time: {total_execution_time} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
