{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6f38843-f34a-40c6-b0e2-c308b44fe57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "#===============================================================================\n",
    "#\n",
    "#           FILE: bigramMNB_4_ntb.py \n",
    "#         AUTHOR: Bianca Ciobanica\n",
    "#\t       EMAIL: bianca.ciobanica@student.uclouvain.be\n",
    "#\n",
    "#           BUGS: \n",
    "#        VERSION: 3.10.6\n",
    "#        CREATED: 25-10-2023 \n",
    "#\n",
    "#===============================================================================\n",
    "#    DESCRIPTION: sources used for this code : \n",
    "#               https://medium.com/@johnm.kovachi/implementing-a-multinomial-naive-bayes-classifier-from-scratch-with-python-e70de6a3b92e\n",
    "#               https://pandas.pydata.org/docs/getting_started/intro_tutorials/03_subset_data.html\n",
    "#               https://datasciencedojo.com/blog/naive-bayes-from-scratch-part-1/#\n",
    "# \n",
    "#    \n",
    "#          USAGE: \n",
    "#==============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfc87cda-5fc0-473f-8e48-051145c83253",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.lm import Vocabulary\n",
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "from math import log, log2\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1adbef5a-4c3c-4a6a-b04c-3d957560d5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_tags(text):\n",
    "  import re\n",
    "  clean = re.compile('<.*?>')\n",
    "  return re.sub(clean, '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5276115c-3160-48a0-9c11-773ecece4cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_tokens_to_UNK(tokens,v):\n",
    "    return [token if token in v else '<UNK>' for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2f6bebb-a4b6-480f-8c60-261e5b74678f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocabulary(df):\n",
    "        tokenized_corpus = [token \n",
    "                            for row in df\n",
    "                            for token in row] # flattened corpus\n",
    "      \n",
    "        return Vocabulary(tokenized_corpus, unk_cutoff=unk_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84f2f66b-fb66-4d8d-8ee9-8baf9bdad1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize training corpus\n",
    "df_corpus = pd.read_csv(\"data/train.csv\")\n",
    "unk_threshold = 3\n",
    "\n",
    "# preprocess \n",
    "df_corpus['Body'] = df_corpus['Body'].apply(lambda x: remove_html_tags(x))\n",
    "df_corpus['Body_tokenized'] = df_corpus['Body'].apply(lambda x: WordPunctTokenizer().tokenize(x))\n",
    "\n",
    "# add_padding\n",
    "n_order = 2\n",
    "df_corpus['Body_tokenized_padded'] = df_corpus['Body_tokenized'].apply(lambda x: list(pad_both_ends(x, n=n_order)))\n",
    "\n",
    "# restricted voc \n",
    "training_padded_v = create_vocabulary(df_corpus['Body_tokenized_padded'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa0c0933-68a2-4eac-8ec0-746f1c7f80ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map UNK words\n",
    "df_corpus['Body_tokenized_padded_UNK'] = df_corpus['Body_tokenized_padded'].apply(lambda x: map_tokens_to_UNK(x, training_padded_v))\n",
    "\n",
    "# add bigrams\n",
    "df_corpus['Body_padded_bigrams'] = [list(ngrams(tokens, n_order))\n",
    "                                       for tokens in df_corpus['Body_tokenized_padded_UNK']\n",
    "                                      ]\n",
    "\n",
    "df_bigrams_flattened = [bigram for row in df_corpus['Body_padded_bigrams'] for bigram in row] # with restricted voc\n",
    "df_tokens_flattened = [t for row in df_corpus['Body_tokenized_padded_UNK'] for t in row] # with restricted voc\n",
    "\n",
    "df_bigrams_counter = Counter(df_bigrams_flattened)\n",
    "df_unigrams_counter = Counter(df_tokens_flattened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7dcd5cb-22ce-4695-8791-fe5bbe230227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Body', 'Y', 'Body_tokenized', 'Body_tokenized_padded',\n",
      "       'Body_tokenized_padded_UNK', 'Body_padded_bigrams'],\n",
      "      dtype='object')\n",
      "(14000, 6)\n"
     ]
    }
   ],
   "source": [
    "print(df_corpus.keys())\n",
    "print(df_corpus.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd6a70c1-9588-4d62-8eff-20128fe2e986",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_vocab_on_bigrams():\n",
    "    # asked chatGPT to create this list of bigrams for my test \n",
    "    v = create_vocabulary([\n",
    "            [('when', 'you'), ('you', 'are'), ('are', 'happy')],\n",
    "            [('i', 'love'), ('love', 'programming'), ('programming', 'so'), ('so', 'much')],\n",
    "            [('this', 'is'), ('is', 'a'), ('a', 'test')],\n",
    "            [('when', 'you'), ('you', 'are'), ('are', 'sad')],\n",
    "        ])\n",
    "    return v\n",
    "#testv = test_vocab_on_bigrams()\n",
    "#print(testv[('you', 'are')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d532801-9a8c-4c42-a678-21441e3340ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_counts = len(df_corpus['Y'])\n",
    "classes = df_corpus['Y'].unique().tolist()\n",
    "#print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e94b6af3-4c99-43ca-a651-58f747f7a158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# P(class | doc) = (P(doc | class) * P(class)) / P(doc)\n",
    "# p(  x   |  y ) = (P( y  |  x   ) * P( x ) ) / P( y )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef244b79-be75-46f4-ba05-3b75185963ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayesClassifier:\n",
    "    # initally created a NB model which i would then initialize per class\n",
    "    # But then realized i should create a model which iterates through each class as shown in SLP\n",
    "    \n",
    "    def __init__(self, docs, classes, alpha=0, voc = None):\n",
    "        self.bigdoc = docs\n",
    "        self.classes = classes\n",
    "        self.alpha = alpha\n",
    "        self.bigdoc_v = voc # Counter object\n",
    "        self.bigdoc_v_size = self.bigdoc_v.total()\n",
    "        \n",
    "        self.bow = {}\n",
    "        self.logprior = {}\n",
    "        self.logll = {}\n",
    "       \n",
    "    def train(self):\n",
    "        if self.bigdoc_v is None or len(self.bigdoc_v) == 0:\n",
    "            raise ValueError(\"Cannot train on an empty vocabulary.\")\n",
    "        \n",
    "        total_doc_counts = len(self.bigdoc)\n",
    "        # go through each class\n",
    "        for label in self.classes:\n",
    "            # get features from class\n",
    "            class_docs = self.bigdoc[self.bigdoc['Y'] == label]\n",
    "            class_counts = len(class_docs)\n",
    "            \n",
    "            # get P(c)\n",
    "            self.logprior[label] = log(class_counts / total_doc_counts)\n",
    "            \n",
    "            # generate bag of words\n",
    "            self.bow[label] = self.extract_features(class_docs)\n",
    "            logll = 0\n",
    "            \n",
    "            # calculate log likelihood\n",
    "            class_bow = self.bow[label]\n",
    "            total_class_tokens = class_bowl.total()\n",
    "            self.logll[label] = {}\n",
    "            \n",
    "            for bigram in df_bigrams_flattened:\n",
    "                bigram_prob = class_bow.get(bigram,0)\n",
    "                self.logll[label][token] = log(( bigram_prob + self.alpha) / (total_class_tokens + self.bigdoc_v_size * self.alpha))\n",
    "\n",
    "    def check_consistency(self):\n",
    "        probs = {}\n",
    "        for label in self.classes:\n",
    "            p = 0\n",
    "            for token in self.probability[label]:\n",
    "                p += self.probability[label][token]\n",
    "            probs[label] = p\n",
    "            \n",
    "        print(probs)\n",
    "        \n",
    "        is_consistent = False\n",
    "        p_class1, p_class2 = probs.values()\n",
    "        if round(p_class1) == 1 and round (p_class2) == 1:\n",
    "            is_consistent = True\n",
    "        return \"Is consistent : \" + str(is_consistent)\n",
    "        \n",
    "    def test_model(self, test_doc):\n",
    "        sums = {} # {\"class\" : logprior}\n",
    "        #C_NB = argmax (logprior + sum logll)\n",
    "        for label in self.classes:\n",
    "            v = self.bow[label]\n",
    "            sums[label] = self.logprior[label]\n",
    "            # go through each word\n",
    "            for token in test_doc:\n",
    "                if token not in v:\n",
    "                    continue\n",
    "                sums[label] = sums[label] + self.logll[label][token]\n",
    "        \n",
    "        # get class with highest score\n",
    "        argmax = max(sums, key=sums.get)\n",
    "        return argmax\n",
    "\n",
    "    def test_accuracy(self, data=None, data_type=None):\n",
    "        if data is None or data.empty:\n",
    "            raise ValueError(\"Cannot train on an empty corpus.\")\n",
    "            \n",
    "        correct_predictions = 0;\n",
    "        total_predictions = 0;\n",
    "    \n",
    "        for index, row in data.iterrows(): # used GPT to help me find how to iter on rows in my df\n",
    "            true_label = row['Y']\n",
    "            predicted_label = self.test_model(row[data_type])\n",
    "            #print(true_label, \"< true =?= predicted >\", predicted_label)\n",
    "    \n",
    "            total_predictions += 1\n",
    "            if predicted_label == true_label:\n",
    "                correct_predictions += 1\n",
    "    \n",
    "        accuracy = correct_predictions / total_predictions\n",
    "        return round(accuracy * 100, 3)\n",
    "            \n",
    "    def __repr__(self):\n",
    "        s = \"\"\n",
    "        s += \"total docs    = \" + str(len(self.bigdoc)) + \"\\n\"\n",
    "        s += \"labels        = \" + str(self.classes)+ \"\\n\"\n",
    "        s+= \"_____________________\\n\"\n",
    "\n",
    "        for label in self.classes:\n",
    "            s += str(label) + \"\\n\"\n",
    "            s += \"P(c)          = \" + str(self.logprior[label]) + \"\\n\"\n",
    "           # s += \"Logll         = \" + str(self.logll[label]) + \"\\n\"\n",
    "            s+= \"_____________________\\n\"\n",
    "        return s \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "23ff84a7-dad8-447b-a951-6d789d37efad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramMultinomialNB(NaiveBayesClassifier):\n",
    "    def __init__(self, docs, classes, alpha = 0, voc = None):\n",
    "        \n",
    "        super().__init__(docs,classes, alpha = alpha, voc = voc)\n",
    "\n",
    "    def extract_features (self, docs):\n",
    "        \"\"\" creates bigram counts per class\n",
    "        \"\"\"\n",
    "        flattened_bigrams = [bigram \n",
    "                          for row in docs['Body_padded_bigrams']\n",
    "                          for bigram in row]\n",
    "        \n",
    "        flattened_unigrams = [token \n",
    "                          for row in docs['Body_tokenized_padded_UNK']\n",
    "                          for token in row]\n",
    "        \n",
    "        bigram_counts = Counter(flattened_bigrams)\n",
    "        unigram_counts = Counter(flattened_unigrams)\n",
    "        \n",
    "        return bigram_counts, unigram_counts\n",
    "\n",
    "    def train(self):\n",
    "        if self.bigdoc_v is None or len(self.bigdoc_v) == 0:\n",
    "            raise ValueError(\"Cannot train on an empty vocabulary.\")\n",
    "        \n",
    "        total_doc_counts = len(self.bigdoc)\n",
    "        # go through each class\n",
    "        for label in self.classes:\n",
    "            # get features from class\n",
    "            class_docs = self.bigdoc[self.bigdoc['Y'] == label]\n",
    "            class_counts = len(class_docs)\n",
    "            \n",
    "            # get P(c)\n",
    "            self.logprior[label] = log(class_counts / total_doc_counts)\n",
    "            \n",
    "            # generate bag of words\n",
    "            self.bow[label] = {}\n",
    "            class_bigrams, class_unigrams = self.extract_features(class_docs)\n",
    "\n",
    "            self.bow[label]['bigrams'] = class_bigrams\n",
    "            self.bow[label]['unigrams'] = class_unigrams\n",
    "            logll = 0\n",
    "            \n",
    "            # calculate log likelihood\n",
    "            self.logll[label] = {}\n",
    "\n",
    "            for bigram in df_bigrams_flattened:\n",
    "                context = bigram[0]\n",
    "                prob_bigram = class_bigrams.get(bigram,0.0)\n",
    "                #print(bigram, bigram_prob)\n",
    "                self.logll[label][bigram] = log((prob_bigram + self.alpha) / (class_unigrams[context] + self.bigdoc_v[context] * self.alpha))\n",
    "\n",
    "    def test_model(self, test_doc):\n",
    "        sums = {} # {\"class\" : logprior}\n",
    "        #C_NB = argmax (logprior + sum logll)\n",
    "        for label in self.classes:\n",
    "            sums[label] = self.logprior[label]\n",
    "            # go through each bigram\n",
    "            for bigram in test_doc:\n",
    "                if bigram in df_bigrams_counter:\n",
    "                    sums[label] += self.logll[label][bigram]\n",
    "                    \n",
    "        # get class with highest score\n",
    "        argmax = max(sums, key=sums.get)\n",
    "        return argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bf03aecb-d674-44bb-b14c-424a9d1c16e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Body', 'Y', 'Body_tokenized', 'Body_tokenized_padded',\n",
      "       'Body_tokenized_padded_UNK', 'Body_padded_bigrams'],\n",
      "      dtype='object')\n",
      "(3500, 6)\n"
     ]
    }
   ],
   "source": [
    "# initialize test corpus and prepocess it\n",
    "test_corpus = pd.read_csv(\"data/test.csv\")\n",
    "# preprocess \n",
    "test_corpus['Body'] = test_corpus['Body'].apply(lambda x: remove_html_tags(x))\n",
    "test_corpus['Body_tokenized'] = test_corpus['Body'].apply(lambda x: WordPunctTokenizer().tokenize(x))\n",
    "test_corpus['Body_tokenized_padded'] = test_corpus['Body_tokenized'].apply(lambda x: list(pad_both_ends(x, n=n_order)))\n",
    "\n",
    "# map unk if oov from training corpus\n",
    "test_corpus['Body_tokenized_padded_UNK'] = test_corpus['Body_tokenized_padded'].apply(lambda x: map_tokens_to_UNK(x, training_padded_v))\n",
    "\n",
    "# add bigrams\n",
    "test_corpus['Body_padded_bigrams'] = [list(ngrams(tokens, n_order))\n",
    "                                       for tokens in test_corpus['Body_tokenized_padded_UNK']\n",
    "                                      ]\n",
    "print(test_corpus.keys())\n",
    "print(test_corpus.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4cf52521-f85c-4f8f-8b40-65ab2f99bb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_perplexity(test_data, alpha):\n",
    "    \"\"\"\n",
    "        testing perplexity on class **unconditional** bigrams\n",
    "        p(w_2 | w_1 ) = P( bigram counts / w_1 counts )\n",
    "    \"\"\"\n",
    "    log_prob_res = 0.0\n",
    "    m = 0\n",
    "    for i in range(1, len(test_data)): # for each question\n",
    "        for bigram in test_data[i]:\n",
    "            # for each token\n",
    "            context_counts = df_unigrams_counter[bigram[0]]\n",
    "            total_tokens = len(df_tokens_flattened)\n",
    "            \n",
    "            log_prob_res += log2( (df_bigrams_counter[bigram] + alpha) / (context_counts + (total_tokens * alpha)))\n",
    "            m += 1\n",
    "        \n",
    "    ll = log_prob_res / m #log likelihood\n",
    "    perplexity = pow(2.0, -ll)\n",
    "    \n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "285bab4e-5579-48d8-afe2-d3f6e964b85a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal alpha : \n",
      " 0.0001\n"
     ]
    }
   ],
   "source": [
    "def optimal_value(l):\n",
    "    res = {}\n",
    "    for alpha in l:\n",
    "        pp = test_perplexity(df_corpus['Body_padded_bigrams'], alpha)\n",
    "        res[alpha] = pp\n",
    "    return min(res, key=res.get)\n",
    "    \n",
    "alpha = optimal_value([0.1, 0.01, 0.001, 0.0001])\n",
    "print(\"Optimal alpha : \\n\",alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "384c12fd-28c9-4cf9-8589-3e543fc897fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total docs    = 14000\n",
      "labels        = ['HQ', 'LQ']\n",
      "_____________________\n",
      "HQ\n",
      "P(c)          = -0.6931471805599453\n",
      "_____________________\n",
      "LQ\n",
      "P(c)          = -0.6931471805599453\n",
      "_____________________\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create binary class conditional model\n",
    "# our voc is the total number of bigrams in training corpus\n",
    "# restricted voc was used to map any oov  token ( with threshold = 3) to '<UNK>'\n",
    "bigramMNB_Model = BigramMultinomialNB(df_corpus, classes, alpha=alpha, voc = df_unigrams_counter) # smoothing according to optimal alpha\n",
    "bigramMNB_Model.train()\n",
    "\n",
    "print(bigramMNB_Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9f01d0e6-ca53-416b-92c6-a5d796b68dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy prediction for bigram MNB on test data :\n",
      " 86.371\n"
     ]
    }
   ],
   "source": [
    "bigramMNB_accuracy = bigramMNB_Model.test_accuracy(test_corpus, data_type='Body_padded_bigrams')\n",
    "print(\"Accuracy prediction for bigram MNB on test data :\\n\", bigramMNB_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
