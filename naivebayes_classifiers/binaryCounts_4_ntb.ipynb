{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b2ede0a-6356-4658-9575-d3cf9e1a3177",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "#===============================================================================\n",
    "#\n",
    "#           FILE: binaryCounts_4_ntb.py\n",
    "#         AUTHOR: Bianca Ciobanica\n",
    "#\t       EMAIL: bianca.ciobanica@student.uclouvain.be\n",
    "#\n",
    "#           BUGS: \n",
    "#        VERSION: 3.10.6\n",
    "#        CREATED: 26-10-2023 \n",
    "#\n",
    "#===============================================================================\n",
    "#    DESCRIPTION: used this ressource to learn how to remove duplicates\n",
    "#                https://www.w3schools.com/python/python_howto_remove_duplicates.asp\n",
    "# \n",
    "#    \n",
    "#          USAGE: \n",
    "#==============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "896fdb4e-31f2-410e-8788-743503a4dc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.lm import Vocabulary\n",
    "from math import log\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd8415b0-80ee-41ed-8be7-63a84a24482e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_tags(text):\n",
    "  import re\n",
    "  clean = re.compile('<.*?>')\n",
    "  return re.sub(clean, '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8317a9e0-33e2-4838-bf1d-5c93eaee32d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocabulary(df):\n",
    "        tokenized_corpus = [token \n",
    "                            for row in df\n",
    "                            for token in row] # flattened corpus\n",
    "      \n",
    "        return tokenized_corpus, Vocabulary(tokenized_corpus, unk_cutoff=unk_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "626d18c5-23f4-47ca-b7d7-7510daf586ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_remove_duplicates():\n",
    "    \"\"\"\n",
    "    for testing list(set(row))\n",
    "    \"\"\"\n",
    "    t = [['banane','banane','pomme','poire','poire','citron','citron'],['kiwi','banane','poire','poire','citron'],['citron','mangue']]\n",
    "    expected_counts = {\n",
    "        'banane': 2,\n",
    "        'pomme': 1,\n",
    "        'poire': 2,\n",
    "        'kiwi': 1,\n",
    "        'citron': 3,\n",
    "        'mangue': 1\n",
    "    }\n",
    "    test = [token for row in t for token in list(set(row))] # get only unique tokens\n",
    "    test_voc = Vocabulary(test, unk_cutoff=1)\n",
    "    print(test)\n",
    "    # used chatGPT for formatted prints\n",
    "    for token, expected_count in expected_counts.items():\n",
    "        if test_voc[token] == expected_count:\n",
    "            print(f\"Test passed! Counted {expected_count} for {token}\")\n",
    "        else:\n",
    "            print(f\"Test failed! Expected {expected_count} for {token}, but got {test_voc[token]}\")\n",
    "\n",
    "    \n",
    "#test_remove_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd3fde37-3955-491c-a0de-a3d7cf45156b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize training corpus\n",
    "df_corpus = pd.read_csv(\"data/train.csv\")\n",
    "unk_threshold = 3\n",
    "# preprocess \n",
    "df_corpus['Body'] = df_corpus['Body'].apply(lambda x: remove_html_tags(x))\n",
    "df_corpus['Body_tokenized'] = df_corpus['Body'].apply(lambda x: WordPunctTokenizer().tokenize(x))\n",
    "# remove duplicates\n",
    "df_corpus['Body_tokenized_unique'] = df_corpus['Body_tokenized'].apply(lambda x: list(set(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98363fd4-5ee7-4d27-b064-a5cdbf4d6305",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_unique, bigdoc_voc = create_vocabulary(df_corpus['Body_tokenized_unique'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0133064a-1e96-4fb1-bd67-e9eeca4e829f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Body', 'Y', 'Body_tokenized', 'Body_tokenized_unique'], dtype='object')\n",
      "(14000, 4)\n"
     ]
    }
   ],
   "source": [
    "print(df_corpus.keys())\n",
    "print(df_corpus.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a80293b0-a03a-46a2-b2ea-4b1b8bd5a3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_counts = len(df_corpus['Y'])\n",
    "classes = df_corpus['Y'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fcc0fb4f-07ee-4df1-8a05-46b58873fae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = df_corpus['Y'].unique().tolist()\n",
    "#print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99607a90-3c06-426d-8df7-6597dc26193a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayesClassifier:\n",
    "    # This code is inspired by code generated with the GPT-3 model developed by OpenAI.\n",
    "    # I initally created a NB model which i would then initialize per class\n",
    "    # But then I realized i should create a model which iterates through each class as shown in SLP\n",
    "    \n",
    "    def __init__(self, docs, classes, alpha=0, voc = None):\n",
    "        self.bigdoc = docs\n",
    "        self.classes = classes\n",
    "        self.alpha = alpha\n",
    "        self.bigdoc_v = voc\n",
    "        self.bigdoc_v_size = self.bigdoc_v.total()\n",
    "        \n",
    "        self.bow = {}\n",
    "        self.logprior = {}\n",
    "        self.logll = {}\n",
    "        self.probability = {}\n",
    "       \n",
    "    def train(self):\n",
    "        if self.bigdoc_v is None or len(self.bigdoc_v) == 0:\n",
    "            raise ValueError(\"Cannot train on an empty vocabulary.\")\n",
    "        \n",
    "        total_doc_counts = len(self.bigdoc)\n",
    "        #print(\"total_doc_counts\",total_doc_counts)\n",
    "        \n",
    "        # go through each class\n",
    "        for label in self.classes:\n",
    "            # get features from class\n",
    "            class_docs = self.bigdoc[self.bigdoc['Y'] == label] # select documents per class\n",
    "            class_counts = len(class_docs)\n",
    "            \n",
    "            # get P(c)\n",
    "            self.logprior[label] = log(class_counts / total_doc_counts)\n",
    "            \n",
    "            # generate bag of words\n",
    "            self.bow[label] = self.extract_features(class_docs)\n",
    "            logll = 0\n",
    "            \n",
    "            # calculate log likelihood\n",
    "            class_bow = self.bow[label]\n",
    "            total_class_tokens = class_bow.total()\n",
    "            self.logll[label] = {}\n",
    "            self.probability[label] = {}\n",
    "            \n",
    "            for token in self.bigdoc_v:\n",
    "                prob_word = class_bow.get(token,0.0)\n",
    "                self.logll[label][token] = log((prob_word + self.alpha) / (total_class_tokens + self.bigdoc_v_size * self.alpha))\n",
    "                if token in class_bow:\n",
    "                    self.probability[label][token] = class_bow[token] / total_class_tokens\n",
    "                \n",
    "    def check_consistency(self):\n",
    "        probs = {}\n",
    "        for label in self.classes:\n",
    "            p = 0\n",
    "            for token in self.probability[label]:\n",
    "                p += self.probability[label][token]\n",
    "            probs[label] = p\n",
    "            \n",
    "        #print(probs)\n",
    "        \n",
    "        is_consistent = False\n",
    "        p_class1, p_class2 = probs.values()\n",
    "        if round(p_class1) == 1 and round (p_class2) == 1:\n",
    "            is_consistent = True\n",
    "        return \"Is consistent : \" + str(is_consistent)\n",
    "        \n",
    "    def test_model(self, test_doc):\n",
    "        sums = {} # {\"class\" : logprior}\n",
    "        #C_NB = argmax (logprior + sum logll)\n",
    "        \n",
    "        for label in self.classes:\n",
    "            sums[label] = self.logprior[label]\n",
    "            # go through each word\n",
    "            for token in test_doc:\n",
    "                if token not in self.bigdoc_v:\n",
    "                    continue\n",
    "                sums[label] = sums[label] + self.logll[label][token]\n",
    "        \n",
    "        # get class with highest score\n",
    "        argmax = max(sums, key=sums.get)\n",
    "        return argmax\n",
    "\n",
    "    def test_accuracy(self, data=None):\n",
    "        if data is None or data.empty:\n",
    "            raise ValueError(\"Cannot train on an empty corpus.\")\n",
    "            \n",
    "        correct_predictions = 0;\n",
    "        total_predictions = 0;\n",
    "    \n",
    "        for index, row in data.iterrows(): # used GPT to help me find how to iter on rows in my df\n",
    "            true_label = row['Y']\n",
    "            predicted_label = self.test_model(row['Body_tokenized'])\n",
    "           # print(true_label, \"< true =?= predicted >\", predicted_label)\n",
    "    \n",
    "            total_predictions += 1\n",
    "            if predicted_label == true_label:\n",
    "                correct_predictions += 1\n",
    "    \n",
    "        accuracy = correct_predictions / total_predictions\n",
    "        return round(accuracy * 100, 3)\n",
    "            \n",
    "    def __repr__(self):\n",
    "        s = \"\"\n",
    "        s += \"total docs    = \" + str(len(self.bigdoc)) + \"\\n\"\n",
    "        s += \"labels        = \" + str(self.classes)+ \"\\n\"\n",
    "        s += \"Voc        = \" + str(self.bigdoc_v_size)+ \"\\n\"\n",
    "        s+= \"_____________________\\n\"\n",
    "\n",
    "        for label in self.classes:\n",
    "            s += str(label) + \"\\n\"\n",
    "            s += \"Voc size    = \" + str(sum(self.bow[label][token] for token in self.bow[label])) + \"\\n\"\n",
    "            s += \"P(c)          = \" + str(self.logprior[label]) + \"\\n\"\n",
    "           # s += \"Logll         = \" + str(self.logll[label]) + \"\\n\"\n",
    "            s+= \"_____________________\\n\"\n",
    "        return s "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0f7391f-58f5-46f9-905f-d3c8bfcbe7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryNB(NaiveBayesClassifier):\n",
    "    def __init__(self, docs, classes, alpha = 0, voc = None):\n",
    "        \n",
    "        super().__init__(docs,classes, alpha = alpha, voc = voc)\n",
    "\n",
    "    def extract_features (self, docs):\n",
    "        \"\"\"creates binary counts per class\n",
    "        \"\"\"\n",
    "        flattened_docs = [token \n",
    "                          for row in docs['Body_tokenized_unique']\n",
    "                          for token in row] # ignore oov tokens\n",
    "        \n",
    "        return Counter(flattened_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7973b17-0738-4c47-8391-eb9ec118504c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Body', 'Y', 'Body_tokenized'], dtype='object')\n",
      "(3500, 3)\n"
     ]
    }
   ],
   "source": [
    "test_corpus = pd.read_csv(\"data/test.csv\")\n",
    "# preprocess \n",
    "test_corpus['Body'] = test_corpus['Body'].apply(lambda x: remove_html_tags(x))\n",
    "test_corpus['Body_tokenized'] = test_corpus['Body'].apply(lambda x: WordPunctTokenizer().tokenize(x))\n",
    "\n",
    "print(test_corpus.keys())\n",
    "print(test_corpus.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73fdff24-ccf0-48aa-82e2-c6833d554473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is consistent : True \n",
      "\n",
      "total docs    = 14000\n",
      "labels        = ['HQ', 'LQ']\n",
      "Voc        = 1172490\n",
      "_____________________\n",
      "HQ\n",
      "Voc size    = 625117\n",
      "P(c)          = -0.6931471805599453\n",
      "_____________________\n",
      "LQ\n",
      "Voc size    = 547373\n",
      "P(c)          = -0.6931471805599453\n",
      "_____________________\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create binary class conditional model\n",
    "\n",
    "tokens_no_dupl_counter = Counter(corpus_unique)\n",
    "\n",
    "binaryNB_Model = BinaryNB(df_corpus, classes, alpha= 1, voc = tokens_no_dupl_counter) # Laplace smoothing\n",
    "binaryNB_Model.train()\n",
    "print(binaryNB_Model.check_consistency(), '\\n')\n",
    "print(binaryNB_Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56d1b9da-9c85-4095-9146-67966f7cbc38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy prediction for binary NB on test data :\n",
      " 84.486\n"
     ]
    }
   ],
   "source": [
    "binaryNB_accuracy = binaryNB_Model.test_accuracy(test_corpus)\n",
    "print(\"Accuracy prediction for binary NB on test data :\\n\", binaryNB_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
