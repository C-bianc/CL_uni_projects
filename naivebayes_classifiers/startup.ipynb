{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5522b97b-2130-4fc6-a84b-417291e5fd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#===============================================================================\n",
    "#\n",
    "#           FILE: my_classifier_1-2_ntb.py \n",
    "#         AUTHOR: Bianca Ciobanica\n",
    "#\t       EMAIL: bianca.ciobanica@student.uclouvain.be\n",
    "#\n",
    "#           BUGS: \n",
    "#        VERSION: 3.10.6\n",
    "#        CREATED: 20-10-2023 \n",
    "#\n",
    "#===============================================================================\n",
    "#    DESCRIPTION:  \n",
    "#    \n",
    "#          USAGE: \n",
    "#==============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b58eec60-b771-4e00-9eea-c3ff59246886",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import nltk\n",
    "import itertools\n",
    "from collections import Counter\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.lm import Vocabulary, MLE, NgramCounter, Laplace\n",
    "from nltk.lm.util import log_base2\n",
    "from nltk.util import ngrams\n",
    "from nltk.lm.preprocessing import pad_both_ends, pad_sequence\n",
    "from nltk.probability import FreqDist, ConditionalFreqDist, MLEProbDist, ConditionalProbDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5be51d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load corpus from data dir\n",
    "#absolute_path = os.path.dirname(__file__)\n",
    "#full_path = os.path.join(absolute_path, \"data\")\n",
    "\n",
    "# filename = sys.argv[1] # give argument\n",
    "df_corpus = pd.read_csv(\"data/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3efcc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_tags(text):\n",
    "  import re\n",
    "  clean = re.compile('<.*?>')\n",
    "  return re.sub(clean, '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0804d751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize\n",
    "df_corpus['Body'] = df_corpus['Body'].apply(lambda x: remove_html_tags(x))\n",
    "df_corpus['Body_tokenized'] = df_corpus['Body'].apply(lambda x: WordPunctTokenizer().tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0c659e8-ff77-4dfe-b8c7-ade7ee652aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_corpus = [token \n",
    "                    for row in df_corpus['Body_tokenized'] \n",
    "                    for token in row] # flattened corpus\n",
    "unk_threshold = 3\n",
    "vocab_data = Vocabulary(tokenized_corpus, unk_cutoff=unk_threshold)\n",
    "fdist = FreqDist(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e39457df-e856-43ed-a191-ce4510443553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35549\n"
     ]
    }
   ],
   "source": [
    "vocab_len = len(vocab_data)\n",
    "#print(vocab['<s>'])\n",
    "print(vocab_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6bf21470-2361-42e8-919b-44cb4e92f49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fdist.plot(20, cumulative=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37013343-dc20-49f4-9e11-2ed6330fbc97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('~', 178), ('~$', 4), ('~*', 3), ('~/', 12), ('~/.', 20), ('~=', 5), ('~[', 113), ('~]#', 3), ('~~', 3), ('\\x7f', 5)]\n"
     ]
    }
   ],
   "source": [
    "sorted_vocab = sorted(vocab_data)\n",
    "last10_tokens = [(word,vocab_data[word]) for word in sorted_vocab[-10:]]\n",
    "\n",
    "print(last10_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "095f72d4-1862-4089-9b2b-5d3c56b33ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('.', 166257), (',', 57372), (':', 52374), ('(', 52179), ('the', 45297), ('I', 40538), (';', 38359), ('to', 36449), ('=', 35217), ('-', 33518)]\n"
     ]
    }
   ],
   "source": [
    "print(fdist.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7d2fdc7-eb70-44ab-a7d8-06160359ab3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oov rate :  3.242 %\n"
     ]
    }
   ],
   "source": [
    "# calculate threshold\n",
    "words_in_vocab = sum(vocab_data[token] for token in vocab_data)\n",
    "total_tokens = len(tokenized_corpus)\n",
    "counts = total_tokens - words_in_vocab\n",
    "\n",
    "#print((counts / len(tokenized_corpus) ) * 100)\n",
    "\n",
    "oov_counts = sum(1 for token in tokenized_corpus if token not in vocab_data)\n",
    "oov_proportion = (oov_counts / len(tokenized_corpus)) * 100\n",
    "rounded_rate = round(oov_proportion, 3)\n",
    "\n",
    "print(\"Oov rate : \", rounded_rate, \"%\")\n",
    "#print(vocab_data['<UNK>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0e93593-cd54-4827-8c72-a9efd1c516ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<s>', 'How'), ('How', 'to'), ('to', 'rename'), ('rename', 'a'), ('a', 'pane'), ('pane', 'in'), ('in', 'tmux'), ('tmux', '?'), ('?', '</s>')]\n"
     ]
    }
   ],
   "source": [
    "# bigrams of the training corpus in the sentence order \n",
    "# with padding and without UNK tokens\n",
    "# print(df_corpus['Body_tokenized'][8198])\n",
    "\n",
    "n_order = 2\n",
    "# generate bigrams\n",
    "all_bigrams = [list(ngrams(pad_both_ends(tokens, n=n_order), n_order))\n",
    "               for tokens in df_corpus['Body_tokenized']\n",
    "              ]\n",
    "padded_bigram_at_8198 = all_bigrams[8198]\n",
    "print(padded_bigram_at_8198)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71821617-2b18-4f52-88e8-51b67390f5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update vocab with padding\n",
    "df_corpus['Body_tokenized_padded'] = df_corpus['Body_tokenized'].apply(\n",
    "    lambda x: list(pad_both_ends(x, n=n_order)))\n",
    "\n",
    "tokenized_padded_corpus = [token \n",
    "                    for row in df_corpus['Body_tokenized_padded'] \n",
    "                    for token in row] # flattened corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fcf5790a-b079-4078-8342-28380813e188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(df_corpus['Body_tokenized_padded']))\n",
    "# print(len(tokenized_corpus))\n",
    "vocab_data = Vocabulary(tokenized_padded_corpus, unk_cutoff=unk_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9bfbde01-011c-41c9-a9dc-1f6cca7eb82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~~~~~~~ get Most likely starter with MLE for bigram model ~~~~~~~\n",
    "padded_bigram_lm = MLE(2,vocab_data) # create MLE model\n",
    "padded_bigram_lm.fit(all_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "14df7d71-f2af-4e2d-8a27-dc7ee41c7bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"vocab_data length = \",len(padded_bigram_lm.vocab))\n",
    "# print(padded_bigram_lm.vocab)\n",
    "# print(padded_bigram_lm.counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a362027f-8e62-412f-a1ff-a607bfdb55d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most likely 3 starters after <s> with MLE \n",
      "{'I': 0.51, 'i': 0.04, 'In': 0.02}\n"
     ]
    }
   ],
   "source": [
    "# get {token:proba} for top 3 using lm.score(token, [context])\n",
    "# sort scores in descending order\n",
    "# [:3] to get last three\n",
    "most_likely_3_starter = {\n",
    "    token: round(padded_bigram_lm.score(token, ['<s>']), 2)\n",
    "    for token in sorted(padded_bigram_lm.vocab, \n",
    "                        key=lambda token: padded_bigram_lm.score(token, ['<s>']), \n",
    "                        reverse=True)[:3]\n",
    "}\n",
    "print(\"most likely 3 starters after <s> with MLE \")\n",
    "print(most_likely_3_starter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e90c87ae-9276-4fe0-8a87-bd70a411043e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Vocabulary with cutoff=3 unk_label='<UNK>' and 35551 items>\n",
      "35551\n"
     ]
    }
   ],
   "source": [
    "# ~~~~~~~ get Most likely starter with Laplace for bigram model ~~~~~~~\n",
    "laplace_smoothed_lm = Laplace(2, vocab_data)\n",
    "laplace_smoothed_lm.fit(all_bigrams)\n",
    "\n",
    "#print(laplace_smoothed_lm.counts)\n",
    "print(laplace_smoothed_lm.vocab)\n",
    "print(len(laplace_smoothed_lm.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e6e57af1-210a-4e9c-94f8-e69b3ae971d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most likely 3 starters after <s> with Laplace \n",
      "{'I': 0.14, 'i': 0.01, 'In': 0.01}\n"
     ]
    }
   ],
   "source": [
    "most_likely_3_starter_Laplace = {\n",
    "    token: round(laplace_smoothed_lm.score(token, ['<s>']), 2)\n",
    "    for token in sorted(laplace_smoothed_lm.vocab, \n",
    "                        key=lambda token: laplace_smoothed_lm.score(token, ['<s>']), \n",
    "                        reverse=True)[:3]\n",
    "}\n",
    "print(\"most likely 3 starters after <s> with Laplace \")\n",
    "print(most_likely_3_starter_Laplace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "86cf878e-11cd-4933-9472-285f11364a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~~~~~ perlplexity test on test set ~~~~~\n",
    "test_corpus = pd.read_csv(\"data/test.csv\")# load test set\n",
    "# preprocess\n",
    "test_corpus['Body'] = test_corpus['Body'].apply(lambda x: remove_html_tags(x))\n",
    "test_corpus['Body_tokenized'] = test_corpus['Body'].apply(lambda x: WordPunctTokenizer().tokenize(x))\n",
    "test_corpus['Body_tokenized_padded'] = test_corpus['Body_tokenized'].apply(lambda x: list(pad_both_ends(x, n=n_order)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9653f30b-36a7-46a8-b574-3cbb492c51b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_corpus_bigrams = [list(ngrams(tokens, n_order))\n",
    "               for tokens in test_corpus['Body_tokenized_padded']\n",
    "                      ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d78909a3-883d-43ce-aa77-38e71857e0a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 14\u001b[0m\n\u001b[1;32m     10\u001b[0m     perplexity \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mpow\u001b[39m(\u001b[38;5;241m2.0\u001b[39m, \u001b[38;5;241m-\u001b[39mll)\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mround\u001b[39m(perplexity,\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlaplace smoothed bigram model perplexity : \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mtest_perplexity\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mlaplace_smoothed_lm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_corpus_bigrams\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[22], line 9\u001b[0m, in \u001b[0;36mtest_perplexity\u001b[0;34m(model, test_data)\u001b[0m\n\u001b[1;32m      6\u001b[0m         log_prob_res \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mlogscore(bigram[\u001b[38;5;241m1\u001b[39m], [bigram[\u001b[38;5;241m0\u001b[39m]])\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;66;03m#m += 1\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m ll \u001b[38;5;241m=\u001b[39m \u001b[43mlog_prob_res\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m \u001b[38;5;66;03m#log likelihood\u001b[39;00m\n\u001b[1;32m     10\u001b[0m perplexity \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mpow\u001b[39m(\u001b[38;5;241m2.0\u001b[39m, \u001b[38;5;241m-\u001b[39mll)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mround\u001b[39m(perplexity,\u001b[38;5;241m3\u001b[39m)\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "def test_perplexity(model, test_data):\n",
    "    log_prob_res = 0.0\n",
    "    m = 0\n",
    "    for i in range(1, len(test_data)): # for each question\n",
    "        for bigram in test_data[i]:    # for each token\n",
    "            log_prob_res += model.logscore(bigram[1], [bigram[0]])\n",
    "            #m += 1\n",
    "        \n",
    "    ll = log_prob_res / m #log likelihood\n",
    "    perplexity = pow(2.0, -ll)\n",
    "    \n",
    "    return round(perplexity,3)\n",
    "        \n",
    "print(\"laplace smoothed bigram model perplexity : \", test_perplexity( laplace_smoothed_lm, test_corpus_bigrams))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
